source:
  format: kafka
  # Options should be in this format <option>=<value>
  options:
    - kafka.bootstrap.servers=localhost:9092
    - kafkaConsumer.pollTimeoutMs=10000
    - subscribe=behavioral-data
    - startingOffsets=latest
    - failOnDataLoss=false

spark:
  master: local[*]
  app-name: EcommerceStreamingEtl
  log-level: ERROR
  # Configuration should be in this format <config>=<value>
  configurations:
  hadoop-configurations:

transform:
  # mode can be GENERIC (all data go to single table)
  #   or CATEGORIZED (data are saved on different tables based on a category column)
  mode: GENERIC
  base-table: ecomm_event
  category_column: event_type

writer:
  destination: hdfs://localhost:9000/user/mikebraga/database/staging
  format: parquet
  trigger-time: 10 seconds
  checkpoint-location: hdfs://localhost:9000/user/mikebraga/database/checkpoint
  partitions:
    - date